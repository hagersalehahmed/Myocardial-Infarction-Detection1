{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CETtDDAAETDH",
        "outputId": "ae9aaa2d-cf30-4398-cd15-2fafe5c0d7ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.1.3-py3-none-any.whl (135 kB)\n",
            "\u001b[K     |████████████████████████████████| 135 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.9.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (7.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.21.6)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.0.10)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 55.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->keras-tuner) (0.8.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (3.0.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (2.14.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.38.3)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.19.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.50.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.13.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (4.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.2)\n",
            "Installing collected packages: jedi, kt-legacy, keras-tuner\n",
            "Successfully installed jedi-0.18.1 keras-tuner-1.1.3 kt-legacy-1.0.4\n"
          ]
        }
      ],
      "source": [
        "pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XjZSolvm8K25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ec7fec-9eb6-40e2-b0ee-18dcc7b05a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint\n",
        "from sklearn.preprocessing import LabelEncoder \n",
        "le = LabelEncoder() \n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_predict,cross_val_score\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, f1_score, recall_score, confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "from kerastuner import HyperModel\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "from keras import layers\n",
        "from keras.layers import Input, Dense, Dropout, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, Embedding, Add\n",
        "\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,GlobalMaxPooling1D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.preprocessing import image\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from tensorflow.keras.backend import clear_session\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlzBnQ_z8PAU"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5zlMlH-QjFz"
      },
      "source": [
        "## Read training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqNfvg4O8K3B"
      },
      "outputs": [],
      "source": [
        "#read train dataset\n",
        "train=  pd.read_csv('/content/drive/MyDrive/Myocardial Infarction Detection/ptbdb/train.csv',encoding='utf-8')\n",
        "test= pd.read_csv('/content/drive/MyDrive/Myocardial Infarction Detection/ptbdb/test.csv',encoding='utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xd6cfsIQ1eS"
      },
      "source": [
        "## convert label to categorical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ufs4Y3mIdU3"
      },
      "outputs": [],
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "X_train, y_train = train.iloc[: , :-1], train.iloc[: , -1]\n",
        "X_test, y_test = test.iloc[: , :-1], test.iloc[: , -1]\n",
        "y_train_categorical = to_categorical(y_train)\n",
        "y_test_categorical=to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tusq7R1v8K3G"
      },
      "outputs": [],
      "source": [
        "print(\"X shape=\" +str(X_train.shape))\n",
        "print(\"y shape=\" +str(y_train.shape))\n",
        "\n",
        "\n",
        "print(\"testX shape=\" +str(X_test.shape))\n",
        "print(\"testy shape=\" +str(y_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZlo8IktUIXx"
      },
      "source": [
        "## Define some of function that are used as matixs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-1WP2Ta8K3K"
      },
      "outputs": [],
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HslHUgQJUgJD"
      },
      "source": [
        "## Optimize and build mode "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD_KMEJaj1_L"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "model_CNN1= load_model('/content/drive/MyDrive/Myocardial Infarction Detection/ptbdb/model/CNN_Layer1.h5', custom_objects={'f1_m': f1_m,  'precision_m': precision_m, 'recall_m': recall_m} )\n",
        "model_CNN2= load_model('/content/drive/MyDrive/Myocardial Infarction Detection/ptbdb/model/CNN_Layer2.h5', custom_objects={'f1_m': f1_m,  'precision_m': precision_m, 'recall_m': recall_m} )\n",
        "model_CNN3= load_model('/content/drive/MyDrive/Myocardial Infarction Detection/ptbdb/model/CNN_Layer3.h5', custom_objects={'f1_m': f1_m,  'precision_m': precision_m, 'recall_m': recall_m} )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz6XKWp8mVaw"
      },
      "outputs": [],
      "source": [
        "members={'CNN1':model_CNN1,'CNN2':model_CNN2,'CNN3':model_CNN3}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkuQlcahpYSY"
      },
      "outputs": [],
      "source": [
        "for key in members:\n",
        "    for i in range(0, len(members[key].layers)-1):\n",
        "        members[key].layers[i].trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRz18OoBkn_p"
      },
      "outputs": [],
      "source": [
        "from numpy import dstack\n",
        "\n",
        "# create stacked model input dataset as outputs from the ensemble\n",
        "def stacked_dataset(members,padded_train, padded_test, y_train,stack_Train, stack_Test):\n",
        "    epoch=20\n",
        "    BATCH_SIZE=1000\n",
        "    for key in members:     \n",
        "        \n",
        "        if key=='CNN1':\n",
        "            members['CNN1'].fit(padded_train, y_train, epochs=epoch, batch_size=BATCH_SIZE)\n",
        "            yhat1 = members['CNN1'].predict(padded_train, verbose=0)\n",
        "            yhat2 = members['CNN1'].predict(padded_test, verbose=0)\n",
        "            \n",
        "       \n",
        "        elif key=='CNN2':\n",
        "\n",
        "            members['CNN2'].fit(padded_train, y_train, epochs=epoch, batch_size=BATCH_SIZE)\n",
        "            yhat1 = members['CNN2'].predict(padded_train,verbose=0)\n",
        "            yhat2 = members['CNN2'].predict(padded_test,verbose=0)\n",
        "            \n",
        "        elif key=='CNN3':\n",
        "\n",
        "            members['CNN3'].fit(padded_train, y_train, epochs=epoch, batch_size=BATCH_SIZE)\n",
        "            yhat1 = members['CNN3'].predict(padded_train,verbose=0)\n",
        "            yhat2 = members['CNN3'].predict(padded_test,verbose=0)\n",
        "\n",
        "\n",
        "        if stack_Train is None:\n",
        "            stack_Train = yhat1\n",
        "        else:\n",
        "            stack_Train = dstack((stack_Train, yhat1))\n",
        "        \n",
        "        if stack_Test is None:\n",
        "            stack_Test = yhat2\n",
        "        else:\n",
        "            stack_Test = dstack((stack_Test, yhat2))\n",
        "            \n",
        "    stack_Train = stack_Train.reshape((stack_Train.shape[0], stack_Train.shape[1]*stack_Train.shape[2]))\n",
        "    stack_Test = stack_Test.reshape((stack_Test.shape[0], stack_Test.shape[1]*stack_Test.shape[2]))\n",
        "    return stack_Train, stack_Test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtRM6MjXmjpT"
      },
      "outputs": [],
      "source": [
        "SVM_DIC={'Accurcy':[], 'Precision': [],'Recall':[],'F1':[]}\n",
        "LR_DIC={'Accurcy':[], 'Precision': [],'Recall':[],'F1':[]}\n",
        "RF_DIC={'Accurcy':[], 'Precision': [],'Recall':[],'F1':[]}\n",
        "\n",
        "scoring = {'accuracy' : make_scorer(accuracy_score), \n",
        "           'precision' : make_scorer(precision_score, average='macro '),\n",
        "           'recall' : make_scorer(recall_score, average='macro '), \n",
        "           'f1_score' : make_scorer(f1_score, average='macro ')}\n",
        "\n",
        "SFold = StratifiedKFold(n_splits=10)\n",
        "stackX_Test=None\n",
        "stackX_Train=None\n",
        "stackX_Train,stackX_Test =stacked_dataset(members,X_train,X_test, y_train_categorical, stackX_Train, stackX_Test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zug6lEb4sJJS"
      },
      "source": [
        "## Evaluating model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WS_QfOeNWGO"
      },
      "outputs": [],
      "source": [
        "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf']} \n",
        "grid_search = GridSearchCV(estimator = SVC(), param_grid = param_grid, cv = SFold) \n",
        "grid_search.fit(stackX_Train, y_train)\n",
        "model_SVM = grid_search.best_estimator_     \n",
        "\n",
        "#scores_Train_SVM = cross_validate(model_SVM, stackX_Train, y_train, scoring=scoring,  cv=SFold)\n",
        "\n",
        "yhat_SVM=model_SVM.predict(stackX_Test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q35hQ65HMfD"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mIgHPCjfKi-"
      },
      "outputs": [],
      "source": [
        "\n",
        "SVM_DIC['Accurcy'].append(round(100*accuracy_score(y_test, yhat_SVM), 2))\n",
        "SVM_DIC['Precision'].append(round(100*precision_score(y_test, yhat_SVM,average='macro'), 2))\n",
        "SVM_DIC['Recall'].append(round(100*recall_score(y_test, yhat_SVM, average='macro'), 2))\n",
        "SVM_DIC['F1'].append(round(100*f1_score(y_test, yhat_SVM, average='macro') , 2))     \n",
        "\n",
        "print(\"SVM\")\n",
        "print(classification_report(y_test, yhat_SVM, target_names=['Normal','Unnormal'])) \n",
        "print(\"SVM Test\")\n",
        "ReultofTest=pd.DataFrame([])\n",
        "ReultofTest=ReultofTest.append({'AccuracyTest' : round(np.mean(SVM_DIC['Accurcy']),2),\n",
        "                                'PrecisionTest':round(np.mean(SVM_DIC['Precision']),2),\n",
        "             'RecallTest' : round(np.mean(SVM_DIC['Recall']),2),'F1Test':round(np.mean(SVM_DIC['F1']),2)}, ignore_index=True)\n",
        "\n",
        "ReultofTest.reindex(['AccuracyTest','PrecisionTest','RecallTest','F1Test'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRw6XFeompUk"
      },
      "outputs": [],
      "source": [
        "\n",
        "param_grid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l2\"], \"solver\":['liblinear','newton-cg']}\n",
        "\n",
        "grid_search_LR = GridSearchCV(estimator = LogisticRegression(), param_grid = param_grid, cv = SFold ) \n",
        "grid_search_LR.fit(stackX_Train, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_LR = grid_search_LR.best_estimator_\n",
        "#scores_Train_LR = cross_validate(model_LR, stackX_Train, y_train, scoring=scoring,  cv=SFold)\n",
        "yhat_LR=model_LR.predict(stackX_Test)\n",
        "\n",
        "LR_DIC['Accurcy'].append(round(100*accuracy_score(y_test, yhat_LR), 2))\n",
        "LR_DIC['Precision'].append(round(100*precision_score(y_test, yhat_LR,average='macro'), 2))\n",
        "LR_DIC['Recall'].append(round(100*recall_score(y_test, yhat_LR, average='macro'), 2))\n",
        "LR_DIC['F1'].append(round(100*f1_score(y_test, yhat_LR, average='macro') , 2))  \n",
        "\n",
        "\n",
        "print(\"LR\")\n",
        "print(classification_report(y_test, yhat_LR, target_names=['Normal','Unnormal'])) \n",
        "print(\"logistic regression Test\")\n",
        "ReultofTest=pd.DataFrame([])\n",
        "ReultofTest=ReultofTest.append({'AccuracyTest' : round(np.mean(LR_DIC['Accurcy']),2),\n",
        "                                'PrecisionTest':round(np.mean(LR_DIC['Precision']),2),\n",
        "             'RecallTest' : round(np.mean(LR_DIC['Recall']),2),'F1Test':round(np.mean(LR_DIC['F1']),2)}, ignore_index=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4UEhKLEh8bpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lmCFM5gI8rz"
      },
      "outputs": [],
      "source": [
        "print(grid_search_LR.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7XmFqKIXazh"
      },
      "outputs": [],
      "source": [
        "#n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [50, 60]\n",
        "\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]# Create the random grid\n",
        "\n",
        "param_grid = {\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "grid_search_RF= GridSearchCV(estimator = RandomForestClassifier(), param_grid = param_grid, cv = SFold)\n",
        "\n",
        "grid_search_RF.fit(stackX_Train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zq1ei777jT2m"
      },
      "outputs": [],
      "source": [
        "model_RF = grid_search_RF.best_estimator_   \n",
        "yhat_RF=model_RF.predict(stackX_Test)\n",
        "  \n",
        "RF_DIC['Accurcy'].append(round(100*accuracy_score(y_test, yhat_RF), 2))\n",
        "RF_DIC['Precision'].append(round(100*precision_score(y_test, yhat_RF,average='macro'), 2))\n",
        "RF_DIC['Recall'].append(round(100*recall_score(y_test, yhat_RF, average='macro'), 2))\n",
        "RF_DIC['F1'].append(round(100*f1_score(y_test, yhat_RF, average='macro') , 2))  \n",
        "\n",
        "print(\"RF\")\n",
        "print(classification_report(y_test, yhat_RF, target_names=['Normal','Artial Premature'])) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfLHow0sDj_W"
      },
      "outputs": [],
      "source": [
        "print(grid_search_RF.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "k_range = list(range(1, 31))\n",
        "param_grid = dict(n_neighbors=k_range)\n",
        "  \n",
        "grid_search_KNN= GridSearchCV(estimator =KNeighborsClassifier(), param_grid = param_grid, cv = SFold)\n",
        "\n",
        "grid_search_KNN.fit(stackX_Train, y_train)"
      ],
      "metadata": {
        "id": "L8JVIiYsZSO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KNN_DIC={'Accurcy':[], 'Precision': [],'Recall':[],'F1':[]}\n",
        "model_KNN = grid_search_KNN.best_estimator_   \n",
        "yhat_KNN=model_KNN.predict(stackX_Test)\n",
        "  \n",
        "KNN_DIC['Accurcy'].append(round(100*accuracy_score(y_test, yhat_KNN), 2))\n",
        "KNN_DIC['Precision'].append(round(100*precision_score(y_test, yhat_KNN,average='macro'), 2))\n",
        "KNN_DIC['Recall'].append(round(100*recall_score(y_test, yhat_KNN, average='macro'), 2))\n",
        "KNN_DIC['F1'].append(round(100*f1_score(y_test, yhat_KNN, average='macro') , 2))  \n",
        "\n",
        "print(\"KNN\")\n",
        "print(classification_report(y_test, yhat_KNN, target_names=['Normal','Artial Premature'])) "
      ],
      "metadata": {
        "id": "5xD0U_ZPZSR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(grid_search_KNN.best_params_)"
      ],
      "metadata": {
        "id": "rYyVe8yhZSUb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}